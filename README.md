# Bachelor-Thesis

## Using Large Language Models in Translationese Classification

Translationese, the unique linguistic characteristics found in translated texts, has garnered awareness in the fields of natural language processing (NLP) and translation studies. The following paper investigates the use of BERT (Bidirectional Encoder Representations from Transformers) for translationese classification, capitalizing on its contextual language modelling capabilities. The methodology we proposed involves fine-tuning a pre-trained BERT model using a large annotated data set of translationese samples. The performance of the BERT-based classifier is assessed against state-of-the-art methods on a data set, showcasing its accuracy in accurately classifying translationese. The results demonstrate improvements over traditional feature-based approaches and other deep learning models. Furthermore, detailed analysis and visualization techniques shed light on the learned representations and highlight the distinctive features captured by BERT. This research contributes to the expanding body of knowledge in translationese classification and demonstrates the efficacy of BERT in this domain. The suggested methodology can be expanded to various NLP tasks, including stylistic analysis, authorship attribution, and text classification in different linguistic domains.
