\begin{abstractpage}

\begin{abstract}{english}

Translationese, the unique linguistic characteristics found in translated texts, has garnered awareness in the fields of natural language processing (NLP) and translation studies. The following paper investigates the use of BERT (Bidirectional Encoder Representations from Transformers) for translationese classification, capitalizing on its contextual language modelling capabilities. The methodology we proposed involves fine-tuning a pre-trained BERT model using a large annotated data set of translationese samples. The performance of the BERT-based classifier is assessed against state-of-the-art methods on a data set, showcasing its accuracy in accurately classifying translationese. The results demonstrate improvements over traditional feature-based approaches and other deep learning models. Furthermore, detailed analysis and visualization techniques shed light on the learned representations and highlight the distinctive features captured by BERT. This research contributes to the expanding body of knowledge in translationese classification and demonstrates the efficacy of BERT in this domain. The suggested methodology can be expanded to various NLP tasks, including stylistic analysis, authorship attribution, and text classification in different linguistic domains.
\end{abstract}

\begin{abstract}{romanian}

Acest studiu explorează utilizarea modelelor lingvistice extinse în clasificarea traducerilor. Termenul "translationese" se referă la caracteristicile și tiparele distinctive care apar în texte traduse. Ne concentrăm în mod specific pe utilizarea modelului BERT (Bidirectional Encoder Representations from Transformers) în detectarea translationese-ului. Lucrarea începe cu o introducere care prezintă contextul și problema de cercetare. Apoi, efectuăm o revizuire a lucrărilor relevante din domeniu. În secțiunea preliminară, oferim o prezentare generală a Transformatorilor, modelului BERT și bibliotecii Hugging Face, care sunt componente esențiale pentru abordarea noastră. Ulterior, detaliem modelul propus pentru detectarea traducerilor, inclusiv arhitectura și variantele acestuia. Secțiunea dedicată studiului experimental discută datele utilizate și prezintă rezultatele obținute prin abordarea noastră. În final, concluzia rezumă descoperirile, evidențiază limitele studiului nostru și sugerează direcții potențiale pentru cercetări viitoare.
\end{abstract}

% \begin{abstract}{english}

% Translationese, the unique linguistic characteristics found in translated texts, has garnered significant attention in the fields of natural language processing (NLP) and translation studies. This paper investigates the use of BERT (Bidirectional Encoder Representations from Transformers) for translationese classification, capitalizing on its contextual language modelling capabilities. The proposed methodology involves fine-tuning a pre-trained BERT model using a large annotated dataset of translationese samples. The performance of the BERT-based classifier is evaluated against state-of-the-art methods on a dataset, showcasing its accuracy in accurately classifying translationese. The results demonstrate significant improvements over traditional feature-based approaches and other deep learning models. Furthermore, detailed analysis and visualization techniques shed light on the learned representations and highlight the distinctive features captured by BERT. This research contributes to the expanding body of knowledge in translationese classification and demonstrates the efficacy of BERT in this domain. The proposed methodology can be extended to various NLP tasks, including stylistic analysis, authorship attribution, and text classification in different linguistic domains.

% \end{abstract}

% \begin{abstract}{romana}
% Rezumat:
% Traducerea, fenomenul lingvistic care implică artefacte și particularități specifice textelor traduse, a atras o atenție semnificativă în domeniul procesării limbajului natural (PLN) și al studiilor de traducere. Detectarea și clasificarea fenomenului de "traducere" poate oferi înțelegere asupra procesului de traducere și poate fi folosită în diverse aplicații de PLN. Această lucrare explorează aplicarea modelului BERT (Bidirectional Encoder Representations from Transformers) în clasificarea textelor în limba traducerilor, profitând de capacitatea sa de modelare a limbajului în context. Propunem o metodologie care implică ajustarea unui model BERT pre-antrenat pe un set mare de date etichetate care conțin exemple de text în limba traducerilor. Evaluăm performanța clasificatorului bazat pe BERT în comparație cu metodele curente de ultimă generație pe mai multe seturi de date și demonstrăm eficacitatea sa în clasificarea exactă a textelor în limba traducerilor. Rezultatele arată că BERT obține îmbunătățiri semnificative în acuratețea clasificării, depășind abordările tradiționale bazate pe caracteristici și alte modele de învățare profundă. În plus, efectuăm analize detaliate și tehnici de vizualizare pentru a obține înțelegere asupra reprezentărilor învățate și evidențiem caracteristicile discriminante capturate de BERT. Descoperirile noastre contribuie la corpul tot mai mare de cercetare în clasificarea textelor în limba traducerilor și evidențiază eficacitatea modelului BERT în acest domeniu. Metodologia propusă poate fi extinsă la diverse sarcini de PLN care implică analiza stilistică, atribuirea autorului și clasificarea textelor în alte domenii lingvistice.

% \end{abstract}

\end{abstractpage}