\chapter{Introduction}

As we further advance into the age of artificial intelligence, an array of debates have emerged, ranging from the ethical implications of AI to its potential to replace human labour in various sectors. One area of AI that has been at the forefront of these discussions is text generation software. These tools, capable of creating content nearly indistinguishable from human writing, have been met with acclaim and criticism. Yet, while these debates continue, one branch of AI remains less contentious but equally compelling â€“ translation software.

The advent of powerful, large language models has transformed various aspects of the natural language processing (NLP) field, including the realm of Translationese Classification \cite{2_translationese}. This area of work is dedicated to studying and analysing translated texts, often referred to as 'translationese,' due to their distinctive characteristics compared to original, non-translated compositions. The study of translationese, despite its academic actuality and relevance, is yet to be exhaustively explored, especially with respect to leveraging modern language models in the process \cite{2_4_translationese}. 

This thesis, titled "Using Large Language Models in Translationese Classification", will contribute to filling this research gap.
Our aim is to contribute to the growing body of knowledge that seeks to improve the accuracy and efficiency of translation software.

There are many factors that will further increase the usage of the translation software in the following decades and thus create higher demand for better language processing models. Among these factors, we can observe the widespread usage of the internet in the 21st century alongside an increasingly multicultural and multilingual work and life environment. These advancements will most likely lead to a society where one main language is used more than the others, but also real-time cross-lingual conversations might become a common occurrence with the help of translation technology.

\section{Background}

Translationese classification, the process of discerning translated texts from native ones, has increased in importance in recent years because of the need for accurate communication, the limitations of automated translation tools, the significance of cultural appropriateness, a growing demand for the global reach of content, and the importance of preserving native languages. As the world becomes more interconnected and multilingual, the demand for translation, and thus, the need for its quality assessment and detection has been rising in numerous fields and industries.

Machine translation (MT)\cite{background}, the usage of machines in the scope of text translation, has been an area of extensive research for over 70 years, evolving with the advent of artificial intelligence and deep learning systems. In an increasingly globalised context, language barriers can limit access to vast amounts of daily information. Machine translation is seen as a potential solution, providing multilingual access to various types of audiences. It also plays a critical role in indirect translations where texts are translated to a target language via a mediating language, helping to reduce operating costs in an ever-increasing multicultural and multilingual world, as well as helping to manage language combinations that are not as common.

Translationese classification, thus, has become a necessary tool to assess the quality and reliability of machine translations, comparing them to human translations. As the translation process involves various strategies, techniques, and equivalence types, identifying translationese becomes an intricate task. This identification has implications in multiple areas, such as forensic linguistics, where identifying the authorship of translated texts is crucial, and legal fields, where detecting instances of plagiarism or copyright infringement is necessary.

Machine learning methods have been applied for translationese classification and have been shown to outperform traditional manual feature engineering approaches. These models can better capture the intricate linguistic patterns and the influences of translation strategies, cultural factors, and language variations. For instance, models such as BERT have demonstrated superior performance in translationese classification tasks, improving upon the limitations of traditional feature engineering methods.

\section{Research Problem}

Research in the field of machine translation (MT) has long been grappling with the task of improving translation accuracy. Traditional MT systems face numerous challenges such as the correct resolution of written text and communication ambiguities in the source and expressing its intended meaning in a concise and easy-to-read manner. Maintaining consistency in domain-specific translations is also a major concern in MT applications.

Recently, Large Language Models (LLMs), such as GPT-4, have demonstrated outstanding performance in natural language processing (NLP) tasks, which includes machine translation. Despite their impressive results, the current research reveals that LLMs perform relatively poorly at the segment level, indicating room for improvement.

A crucial aspect often overlooked in conventional MT systems is the preparatory steps taken by human translators. Professional translators analyze key features of a text such as keywords, topics, and example sentences before translating. Traditional MT systems, which focus primarily on direct source-to-target mapping, need to pay more attention to these preparatory steps. Such steps resemble the procedures of feature engineering in machine learning, which requires creating meaningful variables that capture underlying patterns in data.

This research problem highlights two primary areas. Firstly, it underscores the limitations of conventional feature engineering techniques in effectively capturing the unique patterns, referred to as translationese, in the translated text. Secondly, it underscores the potential of LLMs to improve translation detection accuracy.

The paper aims to tighten te gap between current literature and a need for more robust and scalable approaches to translationese classification. The main objectives of this investigation are:

Firstly, to investigate the limitations of conventional feature engineering procedural approaches in capturing translationese patterns, thereby better defining the problem at hand.

Secondly, to explore the potential of LLMs in enhancing the accuracy of translation detection, focusing on the possibility of using these models in a novel way to overcome the identified limitations of traditional feature engineering.

The significance of this research stems from the potential applications of accurate translation detection in diverse bodies of work such as academic research, international relations, and machine translation technology development.

The research problem of this study is centred around the question: How can we make better use of the capabilities of large language models to better translation detection accuracy? The proposed research seeks to shed light on the problems, aiming to advance the field of machine translation research.


\section{Thesis Structure}


The proposed structure of this thesis is designed to be easy to read and understand. It begins with the title page, followed by the abstract, which is a brief summary of the study, including the topic under study, the methodology used, and the main results and findings.

Chapter 1 provides the reason why the chosen topic is important to study, and offers contextual information about the topic, specifically, translationese and ways to enhance systems related to it.

Chapter 2 is made up of a broad presentation of the already published literature on the subject. It examines current approaches' strengths and weaknesses and identifies existing research gaps.

Chapter 3 consists of the preliminary study used to understand the theory concerning software related to the topic at hand. The section explores a detailed analysis of Transformers, BERT and Hugging Face.

Chapter 4 describes the steps used to apply the theoretical knowledge previously gathered into a practical experiment. The material includes a detailed description of the models and layers used, calculus formulas, and mechanisms behind them.

Chapter 5 presents the experimental research undertaken in detail. It provides a description of how the study was carried out, data exploration, pre-processing procedures, as well as results for two experiments.

Chapter 6 analyzes the data collected in the study and draws conclusions based on the found information. Furthermore, it breaks down the meanings of the conclusions for the broader field of study and its future, as well as provides recommendations.

Lastly, in the bibliography we included all the references cited throughout the thesis, offering a comprehensive list of all the sources consulted during the research process.
