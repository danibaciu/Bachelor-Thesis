{"cells":[{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"1MewTAHYgz5S"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"ExecuteTime":{"start_time":"2023-04-18T15:43:20.647249Z","end_time":"2023-04-18T15:43:20.651571Z"},"id":"60OgKCwNfggc"},"outputs":[],"source":["import os, torch\n","import pandas as pd\n","import time\n","from torch.utils.data import TensorDataset, SequentialSampler, DataLoader\n","from transformers import BertTokenizer, BertModel\n","import numpy as np"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"_1cwWaIGf4UF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.chdir(\"/content/drive/MyDrive/project\")\n","os.listdir('./')"],"metadata":{"id":"0smC95SsgOSR"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"outputs":[],"source":["# If there's a GPU available...\n","if torch.cuda.is_available():\n","\n","    # Tell PyTorch to use the GPU.\n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"metadata":{"ExecuteTime":{"start_time":"2023-04-18T15:37:55.025993Z","end_time":"2023-04-18T15:37:55.030065Z"},"id":"HYxTP02bfggf"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["def get_all_data_from_filename(full_filename):\n","    df = pd.read_csv(full_filename, delimiter='\\t', header=None, names=['iid','src','native_speaker','original','dest','text','direct','label'])\n","    return df"],"metadata":{"ExecuteTime":{"start_time":"2023-04-18T15:37:55.554784Z","end_time":"2023-04-18T15:37:55.559363Z"},"id":"yxRWHOUEfggg"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["def get_text_and_label(df):\n","    return df.text.values[1:], df.label.values[1:]"],"metadata":{"ExecuteTime":{"start_time":"2023-04-18T15:37:56.055427Z","end_time":"2023-04-18T15:37:56.060743Z"},"id":"YjEnjgHZfggg"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["def import_model(name='bert-base-uncased'):\n","    token = BertTokenizer.from_pretrained(name)\n","    mod = BertModel.from_pretrained(name)\n","    return token, mod"],"metadata":{"ExecuteTime":{"start_time":"2023-04-18T15:37:56.618158Z","end_time":"2023-04-18T15:37:56.624057Z"},"id":"AixH3Gv_fggh"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["def create_test_set(tokenizer, sentences, labels):\n","    # print(sentences)\n","    input_ids = []\n","    attention_masks = []\n","\n","    # For every sentence...\n","    for sent in sentences:\n","        # `encode_plus` will:\n","        #   (1) Tokenize the sentence.\n","        #   (2) Prepend the `[CLS]` token to the start.\n","        #   (3) Append the `[SEP]` token to the end.\n","        #   (4) Map tokens to their IDs.\n","        #   (5) Pad or truncate the sentence to `max_length`\n","        #   (6) Create attention masks for [PAD] tokens.\n","        encoded_dict = tokenizer.encode_plus(\n","                            sent,                      # Sentence to encode.\n","                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                            max_length = 256,           # Pad & truncate all sentences.\n","                            pad_to_max_length = True,\n","                            return_attention_mask = True,   # Construct attn. masks.\n","                            return_tensors = 'pt',     # Return pytorch tensors.\n","                       )\n","\n","        # Add the encoded sentence to the list.\n","        input_ids.append(encoded_dict['input_ids'])\n","\n","        # And its attention mask (simply differentiates padding from non-padding).\n","        attention_masks.append(encoded_dict['attention_mask'])\n","\n","    # Convert the lists into tensors.\n","    input_ids = torch.cat(input_ids, 0)\n","    attention_masks = torch.cat(attention_masks, 0)\n","    # print(type(labels))\n","    labels = torch.tensor(np.array([int(i) for i in labels]))\n","\n","    # Set the batch size.\n","    batch_size = 32\n","    print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n","\n","    # Create the DataLoader.\n","    prediction_data = TensorDataset(input_ids, attention_masks, labels)\n","    prediction_sampler = SequentialSampler(prediction_data)\n","    prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n","\n","    return prediction_dataloader"],"metadata":{"ExecuteTime":{"start_time":"2023-04-18T15:38:38.049769Z","end_time":"2023-04-18T15:38:38.053114Z"},"id":"HlFyLos-fggh"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["def run(model, prediction_dataloader):\n","\n","    # Put model in evaluation mode\n","    model.cuda()\n","    model.eval()\n","    cls_output = []\n","    label_output = []\n","\n","    # Predict\n","    for batch in prediction_dataloader:\n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","\n","          # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","          # Telling the model not to compute or store gradients, saving memory and\n","          # speeding up prediction\n","        with torch.no_grad():\n","              # Forward pass, calculate logit predictions.\n","              result = model(b_input_ids,\n","                             attention_mask=b_input_mask,\n","                             return_dict=True)\n","\n","        cls_output.append(result.pooler_output)\n","        label_output.append(b_labels)\n","\n","\n","    cls_output = torch.cat(cls_output, 0)\n","    label_output = torch.cat(label_output, 0)\n","\n","    # print(type(cls_output))\n","    print(f\"CLS shape = {cls_output.shape}\")\n","    print(f\"Labels shape = {label_output.shape}\")\n","    print('    DONE.')\n","    return cls_output, label_output"],"metadata":{"ExecuteTime":{"start_time":"2023-04-18T15:38:38.474503Z","end_time":"2023-04-18T15:38:38.476203Z"},"id":"hDyocMGIfggh"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["def save_files(filename, output, labels):\n","    np.save(filename + '_cls_output.npy', output)\n","    np.save(filename + '_labels.npy', labels)\n","    print(f\"Saved files for filename = {filename}\\n---------------------------\")"],"metadata":{"ExecuteTime":{"start_time":"2023-04-18T15:38:38.993611Z","end_time":"2023-04-18T15:38:38.996490Z"},"id":"QMiEFiZ_fggh"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["data_path = './dataset'\n","output_path = './cls_output'\n","\n","if not os.path.exists(output_path):\n","    os.mkdir(output_path)\n","\n","tokenizer, model = import_model()\n","\n","filename = \"mono_en_es_train.tsv\"\n","if True:\n","    if filename[-3:] == 'tsv':\n","        start = time.time()\n","        print(f\"Working on file = {os.path.join(output_path, filename[:-4])}\")\n","        all_data = get_all_data_from_filename(os.path.join(data_path,filename))\n","        propositions, labels = get_text_and_label(all_data)\n","        prediction_dataloader = create_test_set(tokenizer, propositions, labels)\n","        cls_output, b_labels = run(model, prediction_dataloader)\n","        end = time.time()\n","        print(f\"Duration is {int(end - start)} seconds which is {int((end-start)/60)} minutes\")\n","        save_files(os.path.join(output_path, filename[:-4]), cls_output.to('cpu').numpy(), b_labels.to('cpu').numpy())"],"metadata":{"ExecuteTime":{"start_time":"2023-04-18T15:35:45.204213Z","end_time":"2023-04-18T15:36:02.125763Z"},"pycharm":{"is_executing":true},"id":"ZKU8Tt8cfggi"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["for fil in os.listdir('./cls_output'):\n","  a = np.load(os.path.join('./cls_output', fil))\n","  print(f\"{fil} \\t   \\t= {a.shape}\")"],"metadata":{"ExecuteTime":{"start_time":"2023-04-18T15:42:55.532547Z","end_time":"2023-04-18T15:43:05.545857Z"},"id":"3J8GNWOQfggi"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":[],"metadata":{"id":"ydWhyu67fggi"}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"private_outputs":true,"provenance":[]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}